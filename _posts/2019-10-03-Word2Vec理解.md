---
layout: post
title: "word2vec理解"
date: "2019-10-03"
description: "介绍CBOW和skip-gram两种类型的Word2Vec以及优化方法"
tag: nlp
---

[word2vec](https://github.com/tmikolov/word2vec)是google于2013年推出的一个nlp工具，它能够训练得到每个词的稠密向量。该向量可以用于度量词与词之间的相似性、关联性。word2vec包含两种模型，分别是CBOW(continuous bag of words)和Skip-Gram；两种训练方式，分别是hierachical softmax和negrative sampling。针对word2vec，本文将按以下几点做介绍(由于纯粹是个人记录知识点，有些没表述清楚的地方还望抱歉)：
* word2vec网络结构
* CBOW模型
* Skip-Gram模型
* hierachical softmax训练方法
* negrative sampling训练方法
  
## Word2Vec网络结构
