---
layout: post
title: "word2vec理解"
date: "2019-10-03"
description: "介绍CBOW和skip-gram两种类型的Word2Vec以及优化方法"
tag: nlp
---

[word2vec](https://github.com/tmikolov/word2vec)是google于2013年推出的一个nlp工具，它能够训练得到每个词的稠密向量。该向量可以用于度量词与词之间的相似性、关联性。word2vec包含两种模型，分别是CBOW(continuous bag of words)和Skip-Gram；两种训练方式，分别是hierachical softmax和negrative sampling。针对word2vec，本文将按以下几点做介绍(由于纯粹是个人记录知识点，有些没表述清楚的地方还望抱歉)：
* word2vec概述
* CBOW模型
* Skip-Gram模型
* hierachical softmax训练方法
* negative sampling训练方法
  
## word2vec概述
文本中如何用一个向量去表示一个单词，早期最简单直接的方式是使用one-hot representation的方式，对每个单词构建一个与词典大小一致的向量，其中为1处的索引表示该单词的位置。这种方式，一方面词向量的长度与词典大小有关，存储空间过于冗余；另一方面向量之间的运算结果没有任何意义。

Distributed representation可以解决one-hot representation的上述问题。它的思路是通过训练，将词映射成一个稠密向量。利用这些训练得到的词向量，我们可以较容易的分析词之间的关系，比如一个有趣的结果：

$$\vec{King} - \vec{Man} + \vec{Women} = \vec{Queen}$$


word2vec的网络结构相比于它的前辈NNLM(neural network language model)比较简单。NNLM网络结构如下图所示：

![NNLM网络结构示意图](https://images2015.cnblogs.com/blog/939075/201607/939075-20160719201106732-581954491.png)

该模型可以拆解成两部分理解：
1. 线性的Embedding层。将输入的N-1个one-hot词向量通过大小为(D,V)的矩阵C映射为N-1个分布式的词向量，D为Embedding大小，V为词典大小。
2. 简单的前向网络。包含tanh隐藏层和softmax输出层.

word2vec相比于NNLM做了如下改造：
1. 去除了NNLM的tanh隐藏层，直接由embedding层连接到softmax层；
2. 忽略上下文环境的序列信息：输入的所有词向量汇总到一个Embedding layer；
3. 将中心词的下文纳入上下文环境。

## CBOW模型
CBOW模型是word2vec算法的第一个模型，其网络结果如下图：

![CBOW网络结构示意图](https://images2015.cnblogs.com/blog/939075/201607/939075-20160719201512435-160028706.png)

从数学上看，CBOW模型等价于一个词袋模型的向量乘以一个Embedding矩阵，从而得到一个连续的embedding向量。其本质是由中心词的上下窗口2c个词的特征向量去估计中心词，所以CBOW模型的训练输入是某个词的上下相关词对应的词向量，输出是这个词的词向量。

## Skip-Gram模型
与CBOW模型相反，Skip-Gram模型是用中心词去估计上下文窗口2c个词，其网络结果如下图：

![Skip-Gram网络结构示意图](https://images2015.cnblogs.com/blog/939075/201607/939075-20160719201532560-2134123571.png)

## hierachical softmax训练方法
由于词典大小很大，在最后的softmax层通常会有大量的计算。为了避免计算所有词的softmax概率，word2vec采用了霍夫曼树来替代隐藏层到softmax层的映射。词典的霍夫曼树构建，与一般的霍夫曼树构建一样，只不过叶子节点的权重为叶节点词的频率。

有了霍夫曼树，在计算softmax概率计算时，我们只需要沿着树形结构走即可，如下图所示，我们可以沿着根节点导到叶子节点词w2。

![霍夫曼树](https://images2017.cnblogs.com/blog/1042406/201707/1042406-20170727105752968-819608237.png)

在延霍夫曼树到达叶子节点的过程中，这里采用了二元逻辑回归的方法，如果是左子树则为负类(0)，沿着右子树则是正类(1)，概率估计公式为sigmoid函数，即
$$ P(+)=\frac{1}{1+e^{-x^T\theta}} $$
其中$x$为内部节点的向量，$\theta$是我们需要训练的模型参数，训练目标是我们希望路径节点的概率乘积最大。

## negative sampling训练方法
尽管hierachical softmax相比与原始的softmax在计算效率上有较大改进，但是在面对生僻词时，在霍夫曼树的路径很长，导致模型训练较慢，所以negative sampling训练方法摒弃了霍夫曼树。

假设中心词w，上下文窗口共2c个词，记为context(w)。这里中心词w与context(w)构成一个正例。既然是负采样，这里我们将采样得到neg个与w不同的中心词，如此得到了1+neg个样本。这个过程引出了两个问题：
1. 如何进行负采样？
2. 针对1+neg个样本如何训练？

### 负采样方法
如果词汇表的大小为𝑉,那么我们就将一段长度为1的线段分成𝑉份，每份对应词汇表中的一个词。当然每个词对应的线段长度是不一样的，高频词对应的线段长，低频词对应的线段短。每个词𝑤的线段长度由下式决定：
$$ len(w) = \frac{count(w)^{\frac{3}{4}}}{\sum{count(u)^{\frac{3}{4}}}} $$
此外，在采样前我们将这段长度为1的线段划分为M等分，这里M>>V,保证每个词对应的线段都会划分成对应的小块。而M份中的每一份都会落在某一个词对应的线段上。在采样的时候，我们只需要从𝑀个位置中采样出𝑛𝑒𝑔个位置就行，此时采样到的每一个位置对应到的线段所属的词就是我们的负例词。
![negative sampling](https://images2017.cnblogs.com/blog/1042406/201707/1042406-20170728152731711-1136354166.png)

## 参考引用
> 1. [word2vec原理(一) CBOW与Skip-Gram模型基础](https://www.cnblogs.com/pinard/p/7160330.html)
> 2. [word2vec原理(二) 基于Hierarchical Softmax的模型](https://www.cnblogs.com/pinard/p/7243513.html)
> 3. [word2vec原理(三) 基于Negative Sampling的模型](https://www.cnblogs.com/pinard/p/7249903.html)
> 4. [Word2Vec详解](https://www.cnblogs.com/guoyaohua/p/9240336.html)