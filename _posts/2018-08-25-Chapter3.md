---
layout: post
title: "概率与信息论"
date: "2018-08-25"
description: "花书第三章：概率与信息论"
tag: deep learning
---

* 1、概率分布
* 2、常用概率分布
* 3、信息论

## 概率分布

### 离散型变量和概率质量函数

离散变量的概率分布可用**概率质量函数**(PMF)来描述，一个函数P是随机变量x的PMF，必须满足以下几个条件：

* P的定义域必须是x所有可能状态的集合；
* 对任意x，需满足下面的条件$$0\le P(x)\le 1$$；
* 归一化条件，所有P(x)之和为1.
### 连续变量和概率密度函数
当研究对象是连续型变量时，我们用**概率密度函数**(PDF)来描述，一个函数p是概率密度函数，必须满足以下几个条件：
* p的定义域必须是x所有可能状态的集合；
* 对任意x，需满足$$p(x)\le 0$$，注意并未要求$$p(x)\ge 1$$；
* 对定义域中的x，需满足$$\int p(x)dx = 1$$.

可以对概率密度函数求积分获得点集的真实概率质量。特别是，x落在集合S的概率可以通过$$p(x)$$对这个集合求积分得到。

### 独立性和条件独立性

两个随机变量$$x$$和$$y$$，如果概率分布可以表示成两个因子的形式，并且一个因子只包含$$x$$，另一个因子只包含$$y$$，那么称这两个随机变量是**相互独立**的。
$$\forall x\in \bold{x}, y\in \bold{y}, p(\bold{x}=x, \bold{y}=y)=p(\bold{x}=x)p(\bold{y}=y)$$
如果关于$$x$$和$$y$$的条件概率分布对于$$z$$的每一个值都可以写成乘积的形式，那么它们在给定$$z$$时是**条件独立**的。
$$
\forall x\in \bold{x}, y\in \bold{y}, z\in \bold{z}, p(\bold{x}=x, \bold{y}=y|\bold{z}=z)=p(\bold{x}=x|\bold{z}=z)p(\bold{y}=y|\bold{z}=z)
$$
可以采用一种简化的形式表示独立性和条件独立性：$$x\bot{y}$$表示相互独立，$$x\bot{y}|z$$表示给定$$z$$时的条件独立性。

### 期望、方差和协方差

函数$$f(x)$$关于分布$$P(x)$$的**期望**是指，由$$P$$产生，$$f$$作用于$$x$$时，$$f(x)$$的平均值。

对于离散随机变量，可以通过求和得到：
$$
E_{x\sim P} [f(x)]=\sum_{x} P(x)f(x)
$$
对于连续型随机变量，可以通过求积分得到：
$$
E_{x\sim P} [f(x)]=\int p(x)f(x)dx
$$
期望是线性的。

**方差**是衡量，当我们对x依据它的概率分布进行采样时，随机变量$$x$$的函数值会呈现多大的差异：
$$
Var(f(x)) = E[(f(x) - E[f(x)])^2]
$$

方差的平方根被称为**标准差**。

**协方差**在某种意义上给出了两个变量线性相关性的强度：
$$
Cov(f(x), g(x))=E[(f(x)-E[f(x)])(g(x)-E[g(x)])]
$$


随机向量$$\bold{x}\in R^n$$的协方差矩阵是一个$$n\times n$$的矩阵。

## 常用概率分布

### Bernoulli分布

是单个二值随机变量的分布，由单个参数$$\phi \in [0, 1]$$控制，$$\phi$$给出了变量等于1的概率，具有如下性质
$$
P(x=1)=\phi \\
P(x=0)=1-\phi \\
P(\bold{x}=x)=\phi^x(1-\phi)^{(1-x)} \\
E_x[x]=\phi
Var_x(x)=\phi(1-\phi)
$$

### Multinoulli分布

指单个具有k个不同状态的离散变量的分布，k是有限值。其中每个分量$$p_i$$表示第$$i$$个状态的概率

### 高斯分布

实数上最常用的分布就是**正态分布**，也称为**高斯分布**：
$$
\mathcal N(x;\mu,\sigma^2)=\sqrt{\frac{1}{2\pi\sigma^2}}exp(-\frac{1}{2\sigma^2}(x-\mu)^2)
$$
其中$$E[x]=\mu$$，分布的标准差为$$\sigma$$。

将高斯分布推广到$$R^n$$空间，这种情况下称为**多维正态分布**，它的参数是一个正定对称矩阵$$\Sigma$$：
$$
\mathcal N(x;\mu,\Sigma)=\sqrt{\frac{1}{(2\pi)^ndet(\Sigma)}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))}
$$

### 指数分布和Laplace分布

在深度学习中，经常会需要一个在$$x=0$$点处取得边界点的分布，为实现这一目的，可以使用**指数分布**：
$$
p(x;\lambda)=\lambda e^{-\lambda x}，其中x\ge0
$$
该分布的期望值为$$\frac{1}{\lambda}$$，方差为$$\frac{1}{\lambda^2}$$。

**Laplace分布**允许我们在任意一点$$\mu$$处设置概率质量的峰值：
$$
Laplace(x;\mu,\gamma)=\frac{1}{2\gamma}exp(-\frac{|x-\mu|}{\gamma})
$$

### 分布的混合

混合模型是组合简单概率分布来生成更丰富分布的简单策略，这会用到一个非常重要的概念——**潜变量**，潜变量是我们不能直接观测到的随机变量。

一个非常强大且常见的混合模型是**高斯混合模型**，它的组件$$p(\bold{x}|c=i)$$是高斯分布，每个组件都有各自的参数，均值和协方差矩阵。

### 常用函数

logistic sigmoid函数：
$$
\sigma(x)=\frac{1}{1+exp(-x)}
$$
sigmoid函数在变量取绝对值非常大的正值或负值时会出现**饱和**现象，意味着函数会变得很平。

softplus函数：
$$
\zeta(x)=log(1+exp(x))
$$
softplus函数可以用于生成正太分布的$$\sigma$$参数，因为它的范围是$$(0, \infty)$$。

## 信息论

量化信息应具有以下特点：

* 非常有可能发生的事件，信息量要比较少；
* 较不可能发生的时间，具有更高的信息量；
* 独立事件应具有增量的信息量，例如投掷硬币两次正面朝上传递的信息量，应该是一次投掷硬币正面向上的信息量2倍。

为了满足上述3个性质，定义一个事件的自信息为：
$$
I(x)=-logP(x)
$$
自信息只处理单个事件的输出，可以用**香农熵**来对整个概率分布中的不确定性总量进行量化：
$$
H(x)=E_{x\sim P}[I(x)]=-E_{x\sim P}[logP(x)]
$$
对于同一个随机变量$$x$$有两个单独的概率分布$$P(x)$$和$$Q(x)$$，可以使用**KL散度**来衡量这两个分布的差异：
$$
D_{KL}(P||Q)=E_{x\sim P}[log\frac{P(x)}{Q(x)}]=E_{x\sim P}[logP(x)-log(Q(x))]
$$
